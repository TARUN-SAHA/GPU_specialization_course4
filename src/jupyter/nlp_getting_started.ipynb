{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-getting-started.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook is to classify tweets if it's an emergency situation or a random tweets.**\n",
        "\n",
        "Competition details are given here: https://www.kaggle.com/competitions/nlp-getting-started\n",
        "\n",
        "I'll be using bidirectional LSTM model and use GloVE word embedding for creating this classification model.\n",
        "\n",
        "Will be running this model on both CPU and GPU to compare the performance of the model for 10 iterations."
      ],
      "metadata": {
        "id": "rErs1FIIStVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWwofGRvg01X",
        "outputId": "be5144e3-4df5-46af-8d34-978c711c0b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# Install kaggle to interact with kaggle competition and download dataset.\n",
        "# Download the kaggle.json by creating a new API token from kaggle account.\n",
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip the dataset\n",
        "!kaggle competitions download -c nlp-getting-started\n",
        "!unzip ./nlp-getting-started.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvxyyLhUiDhk",
        "outputId": "98b10c32-5dbf-461f-d054-6dcd84f8506d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading nlp-getting-started.zip to /content\n",
            "  0% 0.00/593k [00:00<?, ?B/s]\n",
            "100% 593k/593k [00:00<00:00, 110MB/s]\n",
            "Archive:  ./nlp-getting-started.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVE word embeddings for later use\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB3Uo1USlQJf",
        "outputId": "ab18ac82-3133-4f56-bbcb-230ed8c282fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-21 21:55:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-08-21 21:55:19--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-08-21 21:55:19--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.99MB/s    in 2m 39s  \n",
            "\n",
            "2022-08-21 21:57:59 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers"
      ],
      "metadata": {
        "id": "myDrf2nJjdOu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify and train and test filepath\n",
        "train_file = './train.csv'\n",
        "test_file = './test.csv'"
      ],
      "metadata": {
        "id": "pfa0gWs8cz80"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Train and Test data from CSV**"
      ],
      "metadata": {
        "id": "ozGIyNCVWrUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Training data\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "with open(train_file) as f:\n",
        "  reader = csv.reader(f, delimiter=',')\n",
        "  next(reader)\n",
        "  for line in reader:\n",
        "    texts.append(line[3])\n",
        "    labels.append(line[4])\n",
        "\n",
        "# Randomly split the train data into train and val dataset with 90:10 ratio\n",
        "texts = np.array(texts)\n",
        "labels = np.asarray(labels, dtype=np.int32)\n",
        "totalLen = len(texts)\n",
        "val_split = int(0.1 * totalLen)\n",
        "val_pos = np.random.randint(totalLen, size=val_split)\n",
        "train_pos = np.setdiff1d(np.arange(totalLen), val_pos)\n",
        "train_sentences, train_labels = texts[train_pos], labels[train_pos]\n",
        "val_sentences, val_labels = texts[val_pos], labels[val_pos]"
      ],
      "metadata": {
        "id": "KhVqTDFZjwff"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Test data\n",
        "test_sentences = []\n",
        "test_ids = []\n",
        "\n",
        "with open(test_file) as f:\n",
        "  reader = csv.reader(f, delimiter=',')\n",
        "  next(reader)\n",
        "  for line in reader:\n",
        "    test_sentences.append(line[3])\n",
        "    test_ids.append(line[0])"
      ],
      "metadata": {
        "id": "DOtGIy12cqed"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating sequences with padding**\n",
        "\n",
        "Convert all sentences to a fixed length, i.e. SEQ_LEN.\n",
        "If sentence length is less than the SEQ_LEN, pad at the end.\n",
        "Or if sentence length is greater than SEQ_LEN, truncate from the end."
      ],
      "metadata": {
        "id": "zI881_SpXLt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "SEQ_LEN = int(np.percentile(lens, 100))"
      ],
      "metadata": {
        "id": "tstcXQ7zo7lN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = layers.TextVectorization(max_tokens=None,\n",
        "                                      output_sequence_length=SEQ_LEN)\n",
        "vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "G8PpcGAhmLMK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a dictionary of word and its corresponding indexes from vocabulary**"
      ],
      "metadata": {
        "id": "KbfwljD1a3qS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "tyjymCLsqZlY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GloVE vector of dimension 100 will be used as word embeddings**"
      ],
      "metadata": {
        "id": "MNYQtjhbW6BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = './glove.6B.100d.txt'\n",
        "embedding_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "    embedding_index[word] = coefs"
      ],
      "metadata": {
        "id": "43FZpjrErPaJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Embedding Matrix from pre-trained vectors**\n",
        "\n",
        "Creating a map for each word in the current vocabulary, if the word is not available in GloVE then keep embedding of that matrix as sequence of zeros."
      ],
      "metadata": {
        "id": "gRTTNxsvX0TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "missed_words = []\n",
        "\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embedding_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    hits += 1\n",
        "  else:\n",
        "    misses += 1\n",
        "    missed_words.append(word)\n",
        "print(f'Converted {hits} words and missed {misses}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W45zv9CCsXLT",
        "outputId": "6e7528e2-816d-4916-f8e1-e495253f5a3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 11741 words and missed 9467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(num_tokens,\n",
        "                                   embedding_dim,\n",
        "                                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                   trainable=False)"
      ],
      "metadata": {
        "id": "iXtyEOfntSul"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating LSTM Model**\n"
      ],
      "metadata": {
        "id": "wjkt7iZJYT3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(lstm_units, output_units=1, output_activation='sigmoid'):\n",
        "  model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(1,), dtype=tf.string),\n",
        "    vectorizer,\n",
        "    embedding_layer,\n",
        "    layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Bidirectional(layers.LSTM(lstm_units)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(units=output_units, activation=output_activation)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "A7CsSbmPtqqW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare dataset for optimization**\n",
        "\n",
        "Convert an array of sentence and label to TF dataset.\n",
        "Shuffle the training dataset."
      ],
      "metadata": {
        "id": "KttdVWuJYZft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels)).shuffle(1024).batch(32).prefetch(-1)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels)).batch(32).prefetch(-1)"
      ],
      "metadata": {
        "id": "iLG5C1rfdax1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, device, epochs=10, model_callbacks=None):\n",
        "  with tf.device(device_name=device):\n",
        "    history = model.fit(train_dataset,\n",
        "              epochs=epochs,\n",
        "              validation_data=val_dataset,\n",
        "              callbacks=model_callbacks)\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "  return history"
      ],
      "metadata": {
        "id": "2SuzMxdF1zph"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select a CPU and a GPU device to train the model**"
      ],
      "metadata": {
        "id": "hb63Ktn0Y1Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CPU_DEVICE = '/cpu:0'\n",
        "GPU_DEVICE = '/gpu:0' if tf.config.list_physical_devices('GPU') else None"
      ],
      "metadata": {
        "id": "uGXhMRZV-cIA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model**\n",
        "\n",
        "First train on a CPU and then train the same model on a GPU to compare the difference in performance.\n",
        "\n",
        "Training only for 10 iterations with same hyper-parameters."
      ],
      "metadata": {
        "id": "kja_1ZPCZCZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on CPU\n",
        "checkpoint_filepath = './checkpoint_cpu'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "model_c = create_model(64, 1, 'sigmoid')\n",
        "model_c.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics=['accuracy'])\n",
        "train_model(model_c, CPU_DEVICE, 10, [model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7dnTnS9PE9a",
        "outputId": "86111fd6-e4b7-44a5-9188-6d7f4c79c83d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "216/216 [==============================] - 22s 73ms/step - loss: 0.9961 - accuracy: 0.5825 - val_loss: 0.5743 - val_accuracy: 0.7162\n",
            "Epoch 2/10\n",
            "216/216 [==============================] - 14s 66ms/step - loss: 0.8225 - accuracy: 0.6544 - val_loss: 0.5068 - val_accuracy: 0.7608\n",
            "Epoch 3/10\n",
            "216/216 [==============================] - 14s 65ms/step - loss: 0.7028 - accuracy: 0.7007 - val_loss: 0.4830 - val_accuracy: 0.7989\n",
            "Epoch 4/10\n",
            "216/216 [==============================] - 14s 66ms/step - loss: 0.6676 - accuracy: 0.7265 - val_loss: 0.4774 - val_accuracy: 0.7976\n",
            "Epoch 5/10\n",
            "216/216 [==============================] - 18s 82ms/step - loss: 0.6437 - accuracy: 0.7294 - val_loss: 0.4744 - val_accuracy: 0.8068\n",
            "Epoch 6/10\n",
            "216/216 [==============================] - 14s 66ms/step - loss: 0.6063 - accuracy: 0.7431 - val_loss: 0.4709 - val_accuracy: 0.8121\n",
            "Epoch 7/10\n",
            "216/216 [==============================] - 14s 65ms/step - loss: 0.5955 - accuracy: 0.7541 - val_loss: 0.4676 - val_accuracy: 0.8068\n",
            "Epoch 8/10\n",
            "216/216 [==============================] - 14s 66ms/step - loss: 0.5602 - accuracy: 0.7619 - val_loss: 0.4665 - val_accuracy: 0.8068\n",
            "Epoch 9/10\n",
            "216/216 [==============================] - 14s 67ms/step - loss: 0.5531 - accuracy: 0.7679 - val_loss: 0.4526 - val_accuracy: 0.8068\n",
            "Epoch 10/10\n",
            "216/216 [==============================] - 14s 66ms/step - loss: 0.5426 - accuracy: 0.7676 - val_loss: 0.4593 - val_accuracy: 0.7976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e6009add0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on GPU\n",
        "checkpoint_filepath = './checkpoint_gpu'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "model_g = None\n",
        "if GPU_DEVICE is not None:\n",
        "  model_g = create_model(64, 1, 'sigmoid')\n",
        "  model_g.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "  history = train_model(model_g, GPU_DEVICE, 10, [model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEsDfe9wetNU",
        "outputId": "91033a5f-e58d-4d68-f754-0435ebb2fe35"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "216/216 [==============================] - 15s 22ms/step - loss: 1.0104 - accuracy: 0.5841 - val_loss: 0.5528 - val_accuracy: 0.7319\n",
            "Epoch 2/10\n",
            "216/216 [==============================] - 3s 16ms/step - loss: 0.8021 - accuracy: 0.6689 - val_loss: 0.5056 - val_accuracy: 0.7661\n",
            "Epoch 3/10\n",
            "216/216 [==============================] - 3s 16ms/step - loss: 0.7257 - accuracy: 0.7022 - val_loss: 0.5061 - val_accuracy: 0.7898\n",
            "Epoch 4/10\n",
            "216/216 [==============================] - 3s 15ms/step - loss: 0.6721 - accuracy: 0.7280 - val_loss: 0.5228 - val_accuracy: 0.7884\n",
            "Epoch 5/10\n",
            "216/216 [==============================] - 4s 17ms/step - loss: 0.6412 - accuracy: 0.7344 - val_loss: 0.4877 - val_accuracy: 0.7950\n",
            "Epoch 6/10\n",
            "216/216 [==============================] - 6s 26ms/step - loss: 0.6183 - accuracy: 0.7452 - val_loss: 0.4972 - val_accuracy: 0.7976\n",
            "Epoch 7/10\n",
            "216/216 [==============================] - 5s 21ms/step - loss: 0.5826 - accuracy: 0.7550 - val_loss: 0.4849 - val_accuracy: 0.8016\n",
            "Epoch 8/10\n",
            "216/216 [==============================] - 4s 19ms/step - loss: 0.5678 - accuracy: 0.7611 - val_loss: 0.4910 - val_accuracy: 0.8134\n",
            "Epoch 9/10\n",
            "216/216 [==============================] - 4s 20ms/step - loss: 0.5531 - accuracy: 0.7699 - val_loss: 0.4835 - val_accuracy: 0.8147\n",
            "Epoch 10/10\n",
            "216/216 [==============================] - 3s 16ms/step - loss: 0.5273 - accuracy: 0.7799 - val_loss: 0.4862 - val_accuracy: 0.8134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model on validation dataset**"
      ],
      "metadata": {
        "id": "X1Sd5tNzcApJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_g.evaluate(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sGW_u6WjPp-",
        "outputId": "5bc45b2b-1981-46e6-e30a-456ddf51272f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 7ms/step - loss: 0.4835 - accuracy: 0.8147\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.48353710770606995, 0.8147174715995789]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_c.evaluate(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X48ax5sRjSJO",
        "outputId": "fd1bf51e-151a-4e64-b7c0-47e966d50909"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 8ms/step - loss: 0.4526 - accuracy: 0.8068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45262742042541504, 0.8068330883979797]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional steps to submit the predicted results to the kaggle competition**"
      ],
      "metadata": {
        "id": "dG5sjhx-Zeh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model_g.predict(test_sentences)"
      ],
      "metadata": {
        "id": "LMN_zuh_jrgv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_labels = tf.cast(tf.squeeze(tf.round(preds)), dtype=tf.int32)"
      ],
      "metadata": {
        "id": "Q2wMdVbvRWF3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'target': test_pred_labels\n",
        "})\n",
        "test_df.to_csv('./submission_gpu.csv', index=False)"
      ],
      "metadata": {
        "id": "sbqHRBI-Re2c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c nlp-getting-started -f ./submission_gpu.csv -m \"gpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZesBmjaT032",
        "outputId": "4071a070-c7e7-4644-e52f-5a38ca5af30a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "100% 22.2k/22.2k [00:02<00:00, 9.11kB/s]\n",
            "Successfully submitted to Natural Language Processing with Disaster Tweets"
          ]
        }
      ]
    }
  ]
}